1 network_structure/fruit_network.py
2
3 import tensorflow as tf
4 import numpy as np
5 from . import utils
6 from utils import constants
7
8 HEIGHT = 100
9 WIDTH = 100
10 # number of channels for an image - jpeg image has RGB
channels
11 CHANNELS = 3
12 # number of channels for the input layer of the
network: HSV + gray scale
13 NETWORK_DEPTH = 4
14
15 batch_size = 60
16 input_size = HEIGHT * WIDTH * NETWORK_DEPTH
17 # probability to keep the values after a training
iteration
18 dropout = 0.8
19
20 # placeholder for input layer
21 X = tf.placeholder(tf.float32 , [None , input_size],
name="X")
22 # placeholder for actual labels
23 Y = tf.placeholder(tf.int64 , [None], name="Y")
24
25 initial_learning_rate = 0.001
26 final_learning_rate = 0.00001
27 learning_rate = initial_learning_rate
38
28
29
30 def conv_net(input_layer):
31 # number of activation maps for each convolutional
layer
32 number_of_act_maps_conv1 = 16
33 number_of_act_maps_conv2 = 32
34 number_of_act_maps_conv3 = 64
35 number_of_act_maps_conv4 = 128
36
37 # number of outputs for each fully connected layer
38 number_of_fcl_outputs1 = 1024
39 number_of_fcl_outputs2 = 256
40
41 input_layer = tf.reshape(input_layer , shape=[-1,
HEIGHT , WIDTH , NETWORK_DEPTH])
42
43 conv1 = utils.conv(input_layer , ’conv1 ’,
kernel_width=5, kernel_height=5,
num_out_activation_maps=
number_of_act_maps_conv1)
44 conv1 = utils.max_pool(conv1 , ’max_pool1 ’,
kernel_height=2, kernel_width=2,
stride_horizontal=2, stride_vertical=2)
45
46 conv2 = utils.conv(conv1 , ’conv2 ’, kernel_width=5,
kernel_height=5, num_out_activation_maps=
number_of_act_maps_conv2)
47 conv2 = utils.max_pool(conv2 , ’max_pool2 ’,
kernel_height=2, kernel_width=2,
stride_horizontal=2, stride_vertical=2)
48
49 conv3 = utils.conv(conv2 , ’conv3 ’, kernel_width=5,
kernel_height=5, num_out_activation_maps=
number_of_act_maps_conv3)
50 conv3 = utils.max_pool(conv3 , ’max_pool3 ’,
kernel_height=2, kernel_width=2,
stride_horizontal=2, stride_vertical=2)
39
51
52 conv4 = utils.conv(conv3 , ’conv4 ’, kernel_width=5,
kernel_height=5, num_out_activation_maps=
number_of_act_maps_conv4)
53 conv4 = utils.max_pool(conv4 , ’max_pool4 ’,
kernel_height=2, kernel_width=2,
stride_horizontal=2, stride_vertical=2)
54
55 flattened_shape = np.prod([s.value for s in conv4.
get_shape() [1:]])
56 net = tf.reshape(conv4 , [-1, flattened_shape],
name="flatten")
57
58 fcl1 = utils.fully_connected(net, ’fcl1 ’,
number_of_fcl_outputs1)
59 fcl1 = tf.nn.dropout(fcl1 , dropout)
60
61 fcl2 = utils.fully_connected(fcl1 , ’fcl2 ’,
number_of_fcl_outputs2)
62 fcl2 = tf.nn.dropout(fcl2 , dropout)
63
64 out = utils.fully_connected(fcl2 , ’out ’, constants
.num_classes , activation_fn=None)
65
66 return out
67
68
69 def update_learning_rate(acc, learn_rate):
70 return max(learn_rate - acc * learn_rate * 0.9,
final_learning_rate)
71
72
73 def build_model():
74 # build the network
75 logits = conv_net(input_layer=X)
76 # apply softmax on the final layer
77 prediction = tf.nn.softmax(logits)
78
40
79 # calculate the loss using the predicted labels vs
the expected labels
80 loss = tf.reduce_mean(tf.nn.
sparse_softmax_cross_entropy_with_logits(logits
=logits , labels=Y))
81 # use adaptive moment estimation optimizer
82 optimizer = tf.train.AdamOptimizer(learning_rate=
learning_rate)
83 train_op = optimizer.minimize(loss=loss)
84
85 # calculate the accuracy for this training step
86 correct_prediction = tf.equal(tf.argmax(prediction
, 1), Y)
87
88 return train_op , loss , correct_prediction